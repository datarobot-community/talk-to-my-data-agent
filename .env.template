# Refer to https://docs.datarobot.com/en/docs/api/api-quickstart/index.html#create-a-datarobot-api-key
# and https://docs.datarobot.com/en/docs/api/api-quickstart/index.html#retrieve-the-api-endpoint
# Can be deleted on a DataRobot codespace
# DATAROBOT_API_TOKEN=
# DATAROBOT_ENDPOINT=

# Required, unless logged in to pulumi cloud. Choose your own alphanumeric passphrase to be used for encrypting pulumi config
PULUMI_CONFIG_PASSPHRASE=1234abc

# Optional: Choose which frontend implementation to use (streamlit or react)
# FRONTEND_TYPE=react

# LLM Configuration:
# Talk to My Data supports multiple flexible LLM options including:
# - LLM Blueprint with LLM Gateway (default)
# - LLM Blueprint with an External LLM
# - Registered model such as an NVIDIA NIM
# - Already Deployed Text Generation model in DataRobot
#
# You can edit the LLM configuration by manually changing which configuration is
# active (recommended option).
# Simply run `ln -sf ../configurations/<chosen_configuration> infra/infra/llm.py`
#
# If you want to do it dynamically however, you can also set it as a configuration value with:
# INFRA_ENABLE_LLM=<chosen_configuration>
# from the list of options in the infra/configurations/llm folder
# Here are some examples of each of those configuration using the dynamic option described above:

# If you want to use an LLM Blueprint with the LLM Gateway (default)
# uncomment this:
# INFRA_ENABLE_LLM=blueprint_with_llm_gateway.py


# If you want to choose an existing LLM Deployment in DataRobot
# uncomment and configure these:
# TEXTGEN_DEPLOYMENT_ID=<your_deployment_id>
# INFRA_ENABLE_LLM=deployed_llm.py
# LLM_DEFAULT_MODEL=<model_name_your_deployment_accepts>  # e.g., azure/gpt-5-1

# For detailed setup instructions, see README.md#configure-LLM_DEFAULT_MODEL.
# Supported external provider prefixes: azure, bedrock, vertex_ai, anthropic.
# LLM_DEFAULT_MODEL="azure/gpt-5-1"  # Example for Azure OpenAI

# If you want to use a Registered Model with an LLM Blueprint
# like an NVIDIA NIM. This also shows how you can adjust the timeout
# in case getting a GPU takes a long time:
# DATAROBOT_TIMEOUT_MINUTES=120
# TEXTGEN_REGISTERED_MODEL_ID='<Your Registered Model ID>'
# INFRA_ENABLE_LLM=registered_model.py

# If you want to configure an LLM with an external LLM provider
# like Azure, Bedrock, Anthropic, or VertexAI (or all 4). Here we provide
# an Azure AI example, see:
# https://docs.datarobot.com/en/docs/gen-ai/playground-tools/deploy-llm.html
# for details on other providers and details:
# INFRA_ENABLE_LLM=blueprint_with_external_llm.py
# LLM_DEFAULT_MODEL="azure/gpt4o"
# LLM_DEFAULT_LLM_ID="azure-openai-gpt-4-o"
# OPENAI_API_VERSION='2024-08-01-preview'
# OPENAI_API_BASE='https://<your_custom_endpoint>.openai.azure.com'
# OPENAI_API_DEPLOYMENT_ID='<your deployment_id>'
# OPENAI_API_KEY='<your_api_key>'


# =========Database ========== #

# To use an inbuilt database (Snowflake, SAP Datasphere, BigQuery), set the following
# `DATAROBOT_CONNECTION_TYPE` variable to `snowflake`, `sap`, or `bigquery`.
# DATAROBOT_CONNECTION_TYPE=

# For Snowflake

# Either password authentication:
# SNOWFLAKE_USER=
# SNOWFLAKE_PASSWORD=
# Or key file authentication:
# SNOWFLAKE_KEY_PATH=

# Common Snowflake settings (required if using Snowflake):
# SNOWFLAKE_ACCOUNT=
# SNOWFLAKE_WAREHOUSE=
# SNOWFLAKE_DATABASE=
# SNOWFLAKE_SCHEMA=
# SNOWFLAKE_ROLE=

# For SAP DataSphere

# SAP_DATASPHERE_HOST=
# SAP_DATASPHERE_PORT=
# SAP_DATASPHERE_USER=
# SAP_DATASPHERE_PASSWORD=
# SAP_DATASPHERE_SCHEMA=


# For BigQuery

# You will need a service account JSON with aiplatform.endpoints.predict permission (https://cloud.google.com/iam/docs/keys-create-delete)
# Add the JSON file content enclosed by '' here:

# GOOGLE_SERVICE_ACCOUNT_BQ=
# GOOGLE_REGION_BQ=
# GOOGLE_DB_SCHEMA_BQ=
