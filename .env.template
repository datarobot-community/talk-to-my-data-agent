# Refer to https://docs.datarobot.com/en/docs/api/api-quickstart/index.html#create-a-datarobot-api-key
# and https://docs.datarobot.com/en/docs/api/api-quickstart/index.html#retrieve-the-api-endpoint
# Can be deleted on a DataRobot codespace
DATAROBOT_API_TOKEN=
DATAROBOT_ENDPOINT=

# Required, unless logged in to pulumi cloud. Choose your own alphanumeric passphrase to be used for encrypting pulumi config
PULUMI_CONFIG_PASSPHRASE=123

# Optional: Choose which frontend implementation to use (streamlit or react)
# FRONTEND_TYPE=react

# Use LLM Gateway instead of DataRobot-hosted LLM (https://docs.datarobot.com/en/docs/gen-ai/genai-code/dr-llm-gateway.html)
# If you select this, you do not need any credentials for your DataRobot-hosted LLM since it will use your DataRobot
# credentials instead.
USE_DATAROBOT_LLM_GATEWAY=

# To use an existing TextGen Model or Deployment:
#  1. Provide exactly one of: registered model ID, or deployment ID 
#  2. Set CHAT_MODEL_NAME to the model name expected by the TextGen model (e.g. gpt-4o-mini)
#  3. Set LLM=LLMs.DEPLOYED_LLM in infra/settings_generative.py

# TEXTGEN_REGISTERED_MODEL_ID=
# TEXTGEN_DEPLOYMENT_ID=
# CHAT_MODEL_NAME=datarobot-deployed-llm # for NIM models, "datarobot-deployed-llm" will automatically be translated to the correct model name.

# Set up DataRobot-hosted LLM deployment (make sure to comment and unset OPENAI_* variables if you want to use DataRobot LLM Gateway)
# For Azure OpenAI:
# Refer to (https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cjavascript-keyless%2Ctypescript-keyless%2Cpython-new&pivots=programming-language-python#retrieve-key-and-endpoint)

# OPENAI_API_KEY=
# OPENAI_API_BASE=
# OPENAI_API_VERSION=
# OPENAI_API_DEPLOYMENT_ID=


# For Google VertexAI

# You will need a service account JSON with aiplatform.endpoints.predict permission (https://cloud.google.com/iam/docs/keys-create-delete)
# Add the JSON file content enclosed by '' here:

# GOOGLE_SERVICE_ACCOUNT=
# GOOGLE_REGION=

# For AWS Bedrock:

# Refer to https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html
# AWS_SESSION_TOKEN is optional in case you are using a long term access key

# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_SESSION_TOKEN=
# AWS_REGION=


# =========Database ========== #

# For Snowflake

# Either password authentication:
# SNOWFLAKE_USER=
# SNOWFLAKE_PASSWORD=
# Or key file authentication:
# SNOWFLAKE_KEY_PATH=

# Common Snowflake settings (required if using Snowflake):
# SNOWFLAKE_ACCOUNT=
# SNOWFLAKE_WAREHOUSE=
# SNOWFLAKE_DATABASE=
# SNOWFLAKE_SCHEMA=
# SNOWFLAKE_ROLE=

# For SAP DataSphere

# SAP_DATASPHERE_HOST=
# SAP_DATASPHERE_PORT=
# SAP_DATASPHERE_USER=
# SAP_DATASPHERE_PASSWORD=
# SAP_DATASPHERE_SCHEMA=


# For BigQuery

# You will need a service account JSON with aiplatform.endpoints.predict permission (https://cloud.google.com/iam/docs/keys-create-delete)
# Add the JSON file content enclosed by '' here:

# GOOGLE_SERVICE_ACCOUNT_BQ=
# GOOGLE_REGION_BQ=
# GOOGLE_DB_SCHEMA_BQ=